{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6781c4bd",
   "metadata": {},
   "source": [
    "# Take-Home Assignment 1 (IS-ML) \n",
    "\n",
    "**Course:** Intelligent Systems – Machine Learning (MICS2-62)\n",
    "\n",
    "**Title:** THA1 — Supervised Learning (Regression) • Classification • Unsupervised (Study)\n",
    "\n",
    "**Group:** 13\n",
    "\n",
    "**Members:** Jose Ignacio Valdivia Aguero, Esteban Leiva Montenegro, Isaac Palma Medina\n",
    "\n",
    "**Spec reference:** Official assignment document\n",
    "- Keep the code for regression models in A **implemented from scratch** (no scikit-learn for the models themselves).\n",
    "- You may use numpy/pandas/matplotlib for data handling and plotting.\n",
    "- For section B (classification), scikit-learn is allowed per the specification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "A. Supervised Learning — Regression\n",
    "1. Data Acquisition (A.I.1)\n",
    "2. Data Transformation (A.I.2)\n",
    "3. Least Squares — Closed Form (A.I.3)\n",
    "4. Linear Regression via Gradient Descent (A.I.4–A.I.8)\n",
    "5. Discussion LS vs GD (A.I.9)\n",
    "6. Polynomial Regression (A.II.1–A.II.4)\n",
    "\n",
    "B. Supervised Learning — Classification (B.I.1–B.I.6)\n",
    "\n",
    "C. Unsupervised Learning — Free Choice Study (C.I.1–C.I.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imports and random seed here\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Supervised Learning — Regression\n",
    "### Dataset Context\n",
    "- Input X: Temperature (°C)\n",
    "- Output Y: Net hourly electrical energy output (MW)\n",
    "\n",
    "$$(13 - 1) \\times 20 + 2 = 242 \\quad\\text{and}\\quad 13 \\times 20 + 1 = 261$$\n",
    "\n",
    "### Dataset from row 242 to 261 (x, y)\n",
    "\n",
    "| **x** | 26 | 23 | 27 | 27 | 19 | 14 | 12 | 18 | 24 | 20 | 22 | 29 | 11 | 17 | 25 | 14 | 26 | 8 | 13 | 17 | \n",
    "|:-----:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| **y** | 440 | 452 | 434 | 437 | 460 | 469 | 469 | 454 | 443 | 446 | 447 | 437 | 477 | 457 | 440 | 466 | 440 | 475 | 467 | 458 | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.1 Data Acquisition\n",
    "- Load the Excel file provided by the course.\n",
    "- Select only your 20 rows using your group number `n`.\n",
    "- Show the selected data in a table (X, Y).\n",
    "- Keep a clean, documented pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = {\n",
    "    \"x\": [26, 23, 27, 27, 19, 14, 12, 18, 24, 20, 22, 29, 11, 17, 25, 14, 26, 8, 13, 17],\n",
    "    \"y\": [440, 452, 434, 437, 460, 469, 469, 454, 443, 446, 447, 437, 477, 457, 440, 466, 440, 475, 467, 458]\n",
    "}\n",
    "print(len(data[\"x\"]), len(data[\"y\"]))  # should both be 20\n",
    "\n",
    "# Load the Excel file and select group-specific rows\n",
    "# file_path = \"./Data Take Home Assignment 1 Exercise A.xlsx\"\n",
    "# n = <enter_group_number>\n",
    "# start = (n - 1) * 20 + 2\n",
    "# end = n * 20 + 1\n",
    "# df = pd.read_excel(file_path)\n",
    "# subset = df.iloc[start-1:end]  # adjust for 0-based indexing\n",
    "# X = subset.iloc[:, 0].to_numpy(dtype=float)\n",
    "# Y = subset.iloc[:, 1].to_numpy(dtype=float)\n",
    "# subset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.2 Data Transformation\n",
    "- Choose and justify a transformation (e.g., Min–Max or Z-score).\n",
    "- Report the formula used in the report and display transformed data here if you apply it for GD.\n",
    "- Keep original X, Y copies for reference.\n",
    "\n",
    "**Formulas:**\n",
    "- Min–Max: \\( x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\)\n",
    "- Z-score: \\( x' = \\frac{x - \\mu}{\\sigma} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a chosen transformation to X and/or Y (if needed for GD stability)\n",
    "# X_t = ...\n",
    "# Y_t = ...\n",
    "# Display summary\n",
    "# print({\"X_mean\": X.mean(), \"X_std\": X.std(), \"Y_mean\": Y.mean(), \"Y_std\": Y.std()})\n",
    "# print({\"X_t_mean\": X_t.mean(), \"X_t_std\": X_t.std(), \"Y_t_mean\": Y_t.mean(), \"Y_t_std\": Y_t.std()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.3 Least Squares — Closed Form\n",
    "Model: \\( h_\\theta(x) = \\theta_1 x + \\theta_0 \\)\n",
    "\n",
    "Closed form solution (OLS): \\( \\theta = (X^T X)^{-1} X^T y \\)\n",
    "\n",
    "Tasks:\n",
    "- Compute \\(\\theta_0\\), \\(\\theta_1\\).\n",
    "- Print parameters and SSE.\n",
    "- Plot data points and fitted line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c536e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_regression_least_squares(X, Y):\n",
    "    \"\"\"\n",
    "    Performs Linear Regression using the analytical Least Squares solution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        1D vector (array) with input values — e.g., temperature in Celsius.\n",
    "    Y : np.ndarray\n",
    "        1D vector (array) with output values — e.g., energy in Megawatts (MW).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta_0 : float\n",
    "        Intercept term of the model (predicted value of Y when X = 0).\n",
    "    theta_1 : float\n",
    "        Slope of the model (average change in Y for each unit change in X).\n",
    "    Y_pred : np.ndarray\n",
    "        Predicted Y values from the fitted regression model.\n",
    "    SSE : float\n",
    "        Sum of Squared Errors, representing total fitting error.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Ensure input data are NumPy arrays (for vectorized math operations)\n",
    "    X = np.array(X, dtype=float)\n",
    "    Y = np.array(Y, dtype=float)\n",
    "    n = len(X)\n",
    "\n",
    "    # Step 2: Compute the means of X and Y\n",
    "    # These represent the \"center\" of the data and will be used for normalization\n",
    "    x_mean = np.mean(X)\n",
    "    y_mean = np.mean(Y)\n",
    "\n",
    "    # Step 3: Compute components for the analytical formulas\n",
    "    # Numerator = covariance(X, Y)\n",
    "    # Denominator = variance(X)\n",
    "    numerador = np.sum((X - x_mean) * (Y - y_mean))\n",
    "    denominador = np.sum((X - x_mean)**2)\n",
    "\n",
    "    # Step 4: Compute the slope (theta_1) and intercept (theta_0)\n",
    "    # Formula: theta_1 = cov(X,Y) / var(X)\n",
    "    #          theta_0 = mean(Y) - theta_1 * mean(X)\n",
    "    theta_1 = numerador / denominador\n",
    "    theta_0 = y_mean - theta_1 * x_mean\n",
    "\n",
    "    # Step 5: Generate predictions for all input values\n",
    "    Y_pred = theta_1 * X + theta_0\n",
    "\n",
    "    # Step 6: Compute the total squared error (SSE)\n",
    "    # This measures the total deviation between real and predicted values\n",
    "    SSE = np.sum((Y - Y_pred)**2)\n",
    "\n",
    "    return theta_0, theta_1, Y_pred, SSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c655d",
   "metadata": {},
   "source": [
    "Plot Linear Regression least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.4 Linear Regression via Gradient Descent — Cost and Gradients\n",
    "Model: \\( h_\\theta(x) = \\theta_1 x + \\theta_0 \\)\n",
    "\n",
    "Cost (MSE): \\( J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (y^{(i)} - h_\\theta(x^{(i)}))^2 \\)\n",
    "\n",
    "Gradients:\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_0} = -\\frac{1}{n} \\sum (y - h) \\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_1} = -\\frac{1}{n} \\sum (y - h) x \\)\n",
    "\n",
    "Update rule: \\( \\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j} \\)\n",
    "\n",
    "Tasks:\n",
    "- Choose learning rate \\(\\alpha\\) and initial parameters.\n",
    "- Implement cost and gradient functions.\n",
    "- Prepare plotting utilities for line and cost curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost J(theta0, theta1), gradients, and a helper to plot the current line\n",
    "# def compute_cost(theta0, theta1, Xv, Yv):\n",
    "#     ...\n",
    "#     return J\n",
    "# def compute_grads(theta0, theta1, Xv, Yv):\n",
    "#     ...\n",
    "#     return dtheta0, dtheta1\n",
    "# def plot_fit(theta0, theta1, Xv, Yv, title):\n",
    "#     ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.5 First GD Iteration\n",
    "- Initialize \\(\\theta_0\\), \\(\\theta_1\\) (random or chosen) and \\(\\alpha\\).\n",
    "- Perform exactly one update step.\n",
    "- Report parameters and plot current line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta0, theta1 = ...  # initial values\n",
    "# alpha = ...\n",
    "# d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "# theta0 = theta0 - alpha * d0\n",
    "# theta1 = theta1 - alpha * d1\n",
    "# print(theta0, theta1)\n",
    "# plot_fit(theta0, theta1, X, Y, \"GD: iteration 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.6 Second GD Iteration\n",
    "- One more update step and plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "# theta0 = theta0 - alpha * d0\n",
    "# theta1 = theta1 - alpha * d1\n",
    "# print(theta0, theta1)\n",
    "# plot_fit(theta0, theta1, X, Y, \"GD: iteration 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.7 Third GD Iteration\n",
    "- One more update step and plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "# theta0 = theta0 - alpha * d0\n",
    "# theta1 = theta1 - alpha * d1\n",
    "# print(theta0, theta1)\n",
    "# plot_fit(theta0, theta1, X, Y, \"GD: iteration 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.8 Last GD Iteration and Cost Curve\n",
    "- Run until a stopping condition (fixed iters or small parameter change).\n",
    "- Plot cost per iteration.\n",
    "- Report final parameters and final fitted line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costs = []\n",
    "# for t in range(...):\n",
    "#     d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "#     theta0 = theta0 - alpha * d0\n",
    "#     theta1 = theta1 - alpha * d1\n",
    "#     costs.append(compute_cost(theta0, theta1, X, Y))\n",
    "# # Plot cost curve\n",
    "# # Plot final fit\n",
    "# print(\"Final:\", theta0, theta1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.9 Discussion — LS vs GD\n",
    "- Compare convergence, numerical stability, sensitivity to scaling, and final fit.\n",
    "- Comment on advantages and limitations for small vs larger datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.II Polynomial Regression (Quadratic)\n",
    "Model: \\( h_\\theta(x) = \\theta_2 x^2 + \\theta_1 x + \\theta_0 \\)\n",
    "\n",
    "Cost: \\( J(\\theta) = \\frac{1}{4n} \\sum_{i=1}^n (y^{(i)} - h_\\theta(x^{(i)}))^4 \\)\n",
    "\n",
    "Gradients (using chain rule): let \\(e = y - h\\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_0} = -\\frac{1}{n} \\sum e^3 \\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_1} = -\\frac{1}{n} \\sum e^3 x \\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_2} = -\\frac{1}{n} \\sum e^3 x^2 \\)\n",
    "\n",
    "Tasks:\n",
    "- Use data from A.I.1 or A.I.2 (justify choice).\n",
    "- Choose learning rate and initial parameters.\n",
    "- Run GD until the model fits well; plot intermediate fits and final curve.\n",
    "- Report initial values, learning rate, and a brief discussion vs linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement polynomial features and GD with the 4th-power cost\n",
    "# def poly_features(x):\n",
    "#     # return [1, x, x^2]\n",
    "#     ...\n",
    "# def poly_cost(theta, Xv, Yv):\n",
    "#     ...\n",
    "# def poly_grads(theta, Xv, Yv):\n",
    "#     ...\n",
    "# # GD loop with plots of intermediate fits\n",
    "# theta = ...  # [theta0, theta1, theta2]\n",
    "# alpha = ...\n",
    "# for t in range(...):\n",
    "#     dtheta = poly_grads(theta, X, Y)\n",
    "#     theta = theta - alpha * dtheta\n",
    "#     # optionally plot selected iterations\n",
    "# print(\"Final theta:\", theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Supervised Learning — Classification\n",
    "Follow the instructions to create two synthetic datasets and compare classifiers (k-NN, Naive Bayes, Decision Trees, Random Forests; optionally SVM/ANN). Use scikit-learn here. Keep code concise and plots clear.\n",
    "\n",
    "## B.I.1 Dataset 1 Creation (Binary, 2D continuous, 40/40)\n",
    "- Generate and display points.\n",
    "- Document data generation approach briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset 1 (binary, two continuous features, 40 points per class)\n",
    "# X1, y1 = ...\n",
    "# Display/plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.2 Dataset 2 (Dataset 1 + Outliers)\n",
    "- Add four outliers per class at random.\n",
    "- Display updated dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset 2 by adding outliers to Dataset 1\n",
    "# X2, y2 = ...\n",
    "# Display/plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.3 Train/Test Split\n",
    "- Justify split strategy and report sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/test split for both datasets\n",
    "# X1_tr, X1_te, y1_tr, y1_te = ...\n",
    "# X2_tr, X2_te, y2_tr, y2_te = ...\n",
    "# print(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.4 Train Models and Report Accuracy + Confusion Matrix\n",
    "- k-NN, Naive Bayes, Decision Tree, Random Forest.\n",
    "- Train on Dataset 1 and 2; evaluate on train and test splits.\n",
    "- Report accuracy and confusion matrix; discuss briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate models (scikit-learn allowed here)\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.5 Decision Boundaries\n",
    "- Visualize decision boundaries for each model on both datasets.\n",
    "- Keep the plotting style simple and consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for each classifier\n",
    "# def plot_decision_boundary(model, Xv, yv, title):\n",
    "#     ...\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.6 Discussion\n",
    "- Compare models and decision boundaries.\n",
    "- Discuss robustness to outliers and dataset differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Unsupervised Learning — Free Choice Study\n",
    "Choose one UL task (e.g., clustering or dimensionality reduction). Keep the study small but coherent, with clear motivations, metrics, and conclusions.\n",
    "\n",
    "## C.I.1 Task and Metrics\n",
    "- State the UL task and justify evaluation metrics.\n",
    "\n",
    "## C.I.2 Data and Splits\n",
    "- Choose a small dataset; describe preprocessing and any split strategy.\n",
    "\n",
    "## C.I.3–C.I.4 Models and Methods\n",
    "- Select two UL models; explain how they work and how they are optimized.\n",
    "\n",
    "## C.I.5 Results and Discussion\n",
    "- Present results clearly; comment on findings.\n",
    "\n",
    "## C.I.6 Conclusions and Limitations\n",
    "- Summarize, list limitations, suggest improvements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
