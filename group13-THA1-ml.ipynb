{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-Home Assignment 1 (IS-ML) \n",
    "\n",
    "**Course:** Intelligent Systems – Machine Learning (MICS2-62)\n",
    "\n",
    "**Title:** THA1 — Supervised Learning (Regression) • Classification • Unsupervised (Study)\n",
    "\n",
    "**Group:** 13\n",
    "\n",
    "**Members:** Jose Ignacio Valdivia Aguero, Esteban Leiva Montenegro, Isaac Palma Medina\n",
    "\n",
    "**Spec reference:** Official assignment document\n",
    "- Keep the code for regression models in A **implemented from scratch** (no scikit-learn for the models themselves).\n",
    "- You may use numpy/pandas/matplotlib for data handling and plotting.\n",
    "- For section B (classification), scikit-learn is allowed per the specification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "A. Supervised Learning — Regression\n",
    "1. Data Acquisition (A.I.1)\n",
    "2. Data Transformation (A.I.2)\n",
    "3. Least Squares — Closed Form (A.I.3)\n",
    "4. Linear Regression via Gradient Descent (A.I.4–A.I.8)\n",
    "5. Discussion LS vs GD (A.I.9)\n",
    "6. Polynomial Regression (A.II.1–A.II.4)\n",
    "\n",
    "B. Supervised Learning — Classification (B.I.1–B.I.6)\n",
    "\n",
    "C. Unsupervised Learning — Free Choice Study (C.I.1–C.I.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Reproducibility\n",
    "- Add imports (numpy, pandas, matplotlib) here.\n",
    "- Set a random seed if needed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imports and random seed here\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Supervised Learning — Regression\n",
    "### Dataset Context\n",
    "- Input X: Temperature (°C)\n",
    "- Output Y: Net hourly electrical energy output (MW)\n",
    "- Use exactly 20 rows according to the group rule: from row `(n−1)*20 + 2` to row `n*20 + 1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.1 Data Acquisition\n",
    "- Load the Excel file provided by the course.\n",
    "- Select only your 20 rows using your group number `n`.\n",
    "- Show the selected data in a table (X, Y).\n",
    "- Keep a clean, documented pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file and select group-specific rows\n",
    "# file_path = \"./Data Take Home Assignment 1 Exercise A.xlsx\"\n",
    "# n = <enter_group_number>\n",
    "# start = (n - 1) * 20 + 2\n",
    "# end = n * 20 + 1\n",
    "# df = pd.read_excel(file_path)\n",
    "# subset = df.iloc[start-1:end]  # adjust for 0-based indexing\n",
    "# X = subset.iloc[:, 0].to_numpy(dtype=float)\n",
    "# Y = subset.iloc[:, 1].to_numpy(dtype=float)\n",
    "# subset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.2 Data Transformation\n",
    "- Choose and justify a transformation (e.g., Min–Max or Z-score).\n",
    "- Report the formula used in the report and display transformed data here if you apply it for GD.\n",
    "- Keep original X, Y copies for reference.\n",
    "\n",
    "**Formulas:**\n",
    "- Min–Max: \\( x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\)\n",
    "- Z-score: \\( x' = \\frac{x - \\mu}{\\sigma} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a chosen transformation to X and/or Y (if needed for GD stability)\n",
    "# X_t = ...\n",
    "# Y_t = ...\n",
    "# Display summary\n",
    "# print({\"X_mean\": X.mean(), \"X_std\": X.std(), \"Y_mean\": Y.mean(), \"Y_std\": Y.std()})\n",
    "# print({\"X_t_mean\": X_t.mean(), \"X_t_std\": X_t.std(), \"Y_t_mean\": Y_t.mean(), \"Y_t_std\": Y_t.std()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.3 Least Squares — Closed Form\n",
    "Model: \\( h_\\theta(x) = \\theta_1 x + \\theta_0 \\)\n",
    "\n",
    "Closed form solution (OLS): \\( \\theta = (X^T X)^{-1} X^T y \\)\n",
    "\n",
    "Tasks:\n",
    "- Compute \\(\\theta_0\\), \\(\\theta_1\\).\n",
    "- Print parameters and SSE.\n",
    "- Plot data points and fitted line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LS parameters from scratch (no sklearn)\n",
    "# theta_1 = ...\n",
    "# theta_0 = ...\n",
    "# Y_hat = ...\n",
    "# SSE = ...\n",
    "# print(\"theta_0:\", theta_0, \"theta_1:\", theta_1, \"SSE:\", SSE)\n",
    "\n",
    "# Plot scatter and regression line\n",
    "# plt.figure()\n",
    "# plt.scatter(X, Y, s=18)\n",
    "# xs = np.linspace(X.min(), X.max(), 200)\n",
    "# ys = theta_1 * xs + theta_0\n",
    "# plt.plot(xs, ys)\n",
    "# plt.xlabel(\"Temperature (°C)\")\n",
    "# plt.ylabel(\"Energy (MW)\")\n",
    "# plt.title(\"Least Squares Fit\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.I.4 Linear Regression via Gradient Descent — Cost and Gradients\n",
    "Model: \\( h_\\theta(x) = \\theta_1 x + \\theta_0 \\)\n",
    "\n",
    "Cost (MSE): \\( J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (y^{(i)} - h_\\theta(x^{(i)}))^2 \\)\n",
    "\n",
    "Gradients:\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_0} = -\\frac{1}{n} \\sum (y - h) \\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_1} = -\\frac{1}{n} \\sum (y - h) x \\)\n",
    "\n",
    "Update rule: \\( \\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j} \\)\n",
    "\n",
    "Tasks:\n",
    "- Choose learning rate \\(\\alpha\\) and initial parameters.\n",
    "- Implement cost and gradient functions.\n",
    "- Prepare plotting utilities for line and cost curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost J(theta0, theta1), gradients, and a helper to plot the current line\n",
    "# def compute_cost(theta0, theta1, Xv, Yv):\n",
    "#     ...\n",
    "#     return J\n",
    "# def compute_grads(theta0, theta1, Xv, Yv):\n",
    "#     ...\n",
    "#     return dtheta0, dtheta1\n",
    "# def plot_fit(theta0, theta1, Xv, Yv, title):\n",
    "#     ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.5 First GD Iteration\n",
    "- Initialize \\(\\theta_0\\), \\(\\theta_1\\) (random or chosen) and \\(\\alpha\\).\n",
    "- Perform exactly one update step.\n",
    "- Report parameters and plot current line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta0, theta1 = ...  # initial values\n",
    "# alpha = ...\n",
    "# d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "# theta0 = theta0 - alpha * d0\n",
    "# theta1 = theta1 - alpha * d1\n",
    "# print(theta0, theta1)\n",
    "# plot_fit(theta0, theta1, X, Y, \"GD: iteration 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.6 Second GD Iteration\n",
    "- One more update step and plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "# theta0 = theta0 - alpha * d0\n",
    "# theta1 = theta1 - alpha * d1\n",
    "# print(theta0, theta1)\n",
    "# plot_fit(theta0, theta1, X, Y, \"GD: iteration 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.7 Third GD Iteration\n",
    "- One more update step and plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "# theta0 = theta0 - alpha * d0\n",
    "# theta1 = theta1 - alpha * d1\n",
    "# print(theta0, theta1)\n",
    "# plot_fit(theta0, theta1, X, Y, \"GD: iteration 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.8 Last GD Iteration and Cost Curve\n",
    "- Run until a stopping condition (fixed iters or small parameter change).\n",
    "- Plot cost per iteration.\n",
    "- Report final parameters and final fitted line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costs = []\n",
    "# for t in range(...):\n",
    "#     d0, d1 = compute_grads(theta0, theta1, X, Y)\n",
    "#     theta0 = theta0 - alpha * d0\n",
    "#     theta1 = theta1 - alpha * d1\n",
    "#     costs.append(compute_cost(theta0, theta1, X, Y))\n",
    "# # Plot cost curve\n",
    "# # Plot final fit\n",
    "# print(\"Final:\", theta0, theta1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.I.9 Discussion — LS vs GD\n",
    "- Compare convergence, numerical stability, sensitivity to scaling, and final fit.\n",
    "- Comment on advantages and limitations for small vs larger datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.II Polynomial Regression (Quadratic)\n",
    "Model: \\( h_\\theta(x) = \\theta_2 x^2 + \\theta_1 x + \\theta_0 \\)\n",
    "\n",
    "Cost: \\( J(\\theta) = \\frac{1}{4n} \\sum_{i=1}^n (y^{(i)} - h_\\theta(x^{(i)}))^4 \\)\n",
    "\n",
    "Gradients (using chain rule): let \\(e = y - h\\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_0} = -\\frac{1}{n} \\sum e^3 \\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_1} = -\\frac{1}{n} \\sum e^3 x \\)\n",
    "- \\( \\frac{\\partial J}{\\partial \\theta_2} = -\\frac{1}{n} \\sum e^3 x^2 \\)\n",
    "\n",
    "Tasks:\n",
    "- Use data from A.I.1 or A.I.2 (justify choice).\n",
    "- Choose learning rate and initial parameters.\n",
    "- Run GD until the model fits well; plot intermediate fits and final curve.\n",
    "- Report initial values, learning rate, and a brief discussion vs linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement polynomial features and GD with the 4th-power cost\n",
    "# def poly_features(x):\n",
    "#     # return [1, x, x^2]\n",
    "#     ...\n",
    "# def poly_cost(theta, Xv, Yv):\n",
    "#     ...\n",
    "# def poly_grads(theta, Xv, Yv):\n",
    "#     ...\n",
    "# # GD loop with plots of intermediate fits\n",
    "# theta = ...  # [theta0, theta1, theta2]\n",
    "# alpha = ...\n",
    "# for t in range(...):\n",
    "#     dtheta = poly_grads(theta, X, Y)\n",
    "#     theta = theta - alpha * dtheta\n",
    "#     # optionally plot selected iterations\n",
    "# print(\"Final theta:\", theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Supervised Learning — Classification\n",
    "Follow the instructions to create two synthetic datasets and compare classifiers (k-NN, Naive Bayes, Decision Trees, Random Forests; optionally SVM/ANN). Use scikit-learn here. Keep code concise and plots clear.\n",
    "\n",
    "## B.I.1 Dataset 1 Creation (Binary, 2D continuous, 40/40)\n",
    "- Generate and display points.\n",
    "- Document data generation approach briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset 1 (binary, two continuous features, 40 points per class)\n",
    "# X1, y1 = ...\n",
    "# Display/plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.2 Dataset 2 (Dataset 1 + Outliers)\n",
    "- Add four outliers per class at random.\n",
    "- Display updated dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset 2 by adding outliers to Dataset 1\n",
    "# X2, y2 = ...\n",
    "# Display/plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.3 Train/Test Split\n",
    "- Justify split strategy and report sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/test split for both datasets\n",
    "# X1_tr, X1_te, y1_tr, y1_te = ...\n",
    "# X2_tr, X2_te, y2_tr, y2_te = ...\n",
    "# print(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.4 Train Models and Report Accuracy + Confusion Matrix\n",
    "- k-NN, Naive Bayes, Decision Tree, Random Forest.\n",
    "- Train on Dataset 1 and 2; evaluate on train and test splits.\n",
    "- Report accuracy and confusion matrix; discuss briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate models (scikit-learn allowed here)\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.5 Decision Boundaries\n",
    "- Visualize decision boundaries for each model on both datasets.\n",
    "- Keep the plotting style simple and consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for each classifier\n",
    "# def plot_decision_boundary(model, Xv, yv, title):\n",
    "#     ...\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.I.6 Discussion\n",
    "- Compare models and decision boundaries.\n",
    "- Discuss robustness to outliers and dataset differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Unsupervised Learning — Free Choice Study\n",
    "Choose one UL task (e.g., clustering or dimensionality reduction). Keep the study small but coherent, with clear motivations, metrics, and conclusions.\n",
    "\n",
    "## C.I.1 Task and Metrics\n",
    "- State the UL task and justify evaluation metrics.\n",
    "\n",
    "## C.I.2 Data and Splits\n",
    "- Choose a small dataset; describe preprocessing and any split strategy.\n",
    "\n",
    "## C.I.3–C.I.4 Models and Methods\n",
    "- Select two UL models; explain how they work and how they are optimized.\n",
    "\n",
    "## C.I.5 Results and Discussion\n",
    "- Present results clearly; comment on findings.\n",
    "\n",
    "## C.I.6 Conclusions and Limitations\n",
    "- Summarize, list limitations, suggest improvements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
